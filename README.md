# ğŸ§¬ Firebase Automation Function for Fetching Genes & Conditions

This project automates the process of fetching **gene information and related conditions** from [MedlinePlus Genetics](https://medlineplus.gov/genetics/gene).  
The data is scraped **A â†’ Z** and stored as **JSON files in Google Cloud Storage (Firebase Storage)**.

---

## ğŸš€ Features
It uses Firebase Cloud Functions (Python) and Google Cloud Storage (GCS) to provide:

### ğŸ§¬ Automated Gene Scraping

Scrapes all gene pages A â†’ Z from MedlinePlus.

Extracts gene names and their associated conditions.

Stores each gene as an individual JSON file in a GCS bucket (genes/all_genes_separate_files/GENE.json).

### âš¡ Cloud Function Triggers

HTTP Function (crawling_genes) â†’ Manually trigger scraping via browser or curl.

##### Pub/Sub Scheduled Function (scheduled_genes_files) â†’ Runs weekly via Cloud Scheduler â†’ Pub/Sub, updating gene data automatically
---

## ğŸ— Project Structure
``` bash 
functions/
â”‚â”€â”€ main.py # Main scraping + Firebase Functions logic
â”‚â”€â”€ requirements.txt # Python dependencies
firebase.json # Firebase configuration
.firebaserc # Project alias for deployment
```

---

## ğŸ”§ Setup & Installation

### 1ï¸âƒ£ Clone the repository
```bash
git clone (https://github.com/Datawith-MdAtaullah/FETCHING-LIVE-DATA-FROM-A-WEBSITE-.git)
cd <repo-name>/functions
```

### 2ï¸âƒ£ Create a virtual environment
``` bash
python -m venv venv
source venv/bin/activate   # Linux / Mac
venv\Scripts\activate      # Windows
```

### 3ï¸âƒ£ Install dependencies

``` bash
pip install -r requirements.txt
```
### 4ï¸âƒ£ Login to Firebase
``` bash 
firebase login
```
### 5ï¸âƒ£ Initialize Firebase Functions (Python)
``` bash
firebase init functions
# Choose Python and existing files
```
### 6ï¸âƒ£ Deploy Functions
``` bash
firebase deploy --only functions
```
---

## âš¡ Usage

### 1. Manual scrape (HTTP trigger)
``` bash
curl https://<region>-<project-id>.cloudfunctions.net/crawling_genes
```

### 2. Weekly scheduled scrape

Runs automatically via Cloud Scheduler â†’ Pub/Sub â†’ scheduled_genes_files

---

## ğŸ“Š Execution Flow

## ğŸ“Š Execution Flow

```mermaid
flowchart TD
    A["Cloud Scheduler (Weekly)"] --> B["Pub/Sub Topic: weekly-crawl-genes"]
    B --> C["Pub/Sub Function: scheduled_genes_files
    - Calls scrape_and_save()
    - Updates JSON in GCS"]

    D["HTTP Function: crawling_genes
    - Manual trigger
    - Calls scrape_and_save()"] --> E["Google Cloud Storage (GCS)
    - Stores gene JSONs"]
```

## ğŸ“ Example Output (per gene JSON)
``` bash
{
  "gene": "BRCA1",
  "conditions": [
    "Breast cancer",
    "Ovarian cancer"
  ]
}
```
## ğŸ“Œ Requirements

Python 3.9+

Firebase CLI installed (npm install -g firebase-tools)

Google Cloud credentials with Storage access

## ğŸ‘¨â€ğŸ’» Author

MD ATAULLAH  â€“ https://github.com/Datawith-MdAtaullah

























